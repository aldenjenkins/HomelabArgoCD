# Enables result streaming from Tempo (to Grafana) via HTTP.
stream_over_http_enabled: true

cache:
  caches:
    - memcached:
        addresses: dnssrvnoa+_memcached-client._tcp.tempo-memcached-discovery.tempo.svc
        consistent_hash: true
        max_idle_conns: 100
        timeout: 1s
        update_interval: 1m
      roles:
        - parquet-footer
        - bloom
        - frontend-search
compactor:
  compaction:
    # block_retention: 240h
    # compacted_block_retention: 1h
    # block_retention: 12h  # How long to keep blocks. Default is 14 days, this demo system is short-lived.
    block_retention: 168h  # How long to keep blocks. Default is 14 days, this demo system is short-lived. 168h = 7 days
    compacted_block_retention: 10m   # How long to keep compacted blocks stored elsewhere.
    # compaction_cycle: 30s
    compaction_cycle: 10m
    compaction_window: 1h  # Blocks in this time window will be compacted together.
    # max_block_bytes: 107374182400
    max_block_bytes: 100_000_000  # (from my old monoservice setup) Maximum size of a compacted block.
    max_compaction_objects: 6000000
    max_time_per_tenant: 5m
    retention_concurrency: 10
    v2_in_buffer_bytes: 5242880
    v2_out_buffer_bytes: 20971520
    v2_prefetch_traces_count: 1000
  ring:
    kvstore:
      store: memberlist
distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  ring:
    kvstore:
      store: memberlist
ingester:
  lifecycler:
    ring:
      kvstore:
        store: memberlist
      replication_factor: 3
      zone_awareness_enabled: false
    tokens_file_path: /var/tempo/tokens.json
memberlist:
  abort_if_cluster_join_fails: false
  bind_addr: []
  bind_port: 7956
  cluster_label: 'tempo.default'
  gossip_interval: 1s
  gossip_nodes: 2
  gossip_to_dead_nodes_time: 30s
  join_members:
    - dns+tempo-gossip-ring:7956
  leave_timeout: 5s
  left_ingesters_timeout: 5m
  max_join_backoff: 1m
  max_join_retries: 10
  min_join_backoff: 1s
  node_name: ""
  packet_dial_timeout: 5s
  packet_write_timeout: 5s
  pull_push_interval: 30s
  randomize_node_name: true
  rejoin_interval: 0s
  retransmit_factor: 2
  stream_timeout: 10s
metrics_generator:
  metrics_ingestion_time_range_slack: 30s
  processor:
    local_blocks:
      flush_to_storage: true
    service_graphs:
      dimensions:
        - homelab.environment
        - homelab.huid
        - homelab.owner
        - homelab.service
      histogram_buckets:
        - 0.1
        - 0.2
        - 0.4
        - 0.8
        - 1.6
        - 3.2
        - 6.4
        - 12.8
      max_items: 50000
      wait: 10s
      workers: 10
    span_metrics:
      dimensions: []
      histogram_buckets:
        - 0.002
        - 0.004
        - 0.008
        - 0.016
        - 0.032
        - 0.064
        - 0.128
        - 0.256
        - 0.512
        - 1.02
        - 2.05
        - 4.1
  registry:
    collection_interval: 15s
    external_labels: {}
    stale_duration: 15m
  ring:
    kvstore:
      store: memberlist
  storage:
    path: /var/tempo/wal
    remote_write:
      - url: http://prometheus-operated.prometheus.svc.cluster.local:9090/api/v1/write
        # TODO: maybe enable this? or maybe this should be done via otel collector?
        # send_exemplars: true
    remote_write_add_org_id_header: true
    remote_write_flush_deadline: 1m
  traces_storage:
    path: /var/tempo/traces
multitenancy_enabled: false
overrides:
  defaults:
    ingestion:
      max_traces_per_user: 0
      rate_limit_bytes: 75000000
    metrics_generator:
      processor:
        service_graphs:
          enable_client_server_prefix: true
      processors:
        - service-graphs
        - local-blocks
  per_tenant_override_config: /runtime-config/overrides.yaml
querier:
  frontend_worker:
    frontend_address: tempo-query-frontend-discovery:9095
  max_concurrent_queries: 20
  search:
    query_timeout: 30s
  trace_by_id:
    query_timeout: 10s
query_frontend:
  max_outstanding_per_tenant: 2000
  max_retries: 2
  metrics:
    max_duration: 96h
  search:
    concurrent_jobs: 1000
    target_bytes_per_job: 104857600
  trace_by_id:
    query_shards: 50
server:
  grpc_server_max_recv_msg_size: 67108864
  grpc_server_max_send_msg_size: 67108864
  http_listen_port: 3200
  http_server_read_timeout: 30s
  http_server_write_timeout: 30s
  log_format: logfmt
  log_level: info
storage:
  trace:
    backend: s3
    # blocklist_poll: 5m
    blocklist_poll: 30m
    local:
      path: /var/tempo/traces
    pool:
      # max_workers: 400
      # queue_depth: 20000
      max_workers: 40
      queue_depth: 2000
    s3:
      bucket: tempo
      endpoint: minio.minio.svc.cluster.local:9000
      access_key: minio
      secret_key: miniominio
      insecure: true
      forcepathstyle: true
    wal:
      path: /var/tempo/wal
usage_report:
  reporting_enabled: false
